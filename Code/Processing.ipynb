{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note: This notebook creates datasets out of the corresponding .txt files for the fantasy datasets\n",
    "\n",
    "!pip install nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Referenced https://pythonspot.com/tokenizing-words-and-sentences-with-nltk/ to see how sent_tokenize works\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Arthur Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing newlines referenced from: https://stackoverflow.com/questions/8369219/how-to-read-a-text-file-into-a-string-variable-and-strip-newlines\n",
    "#\\uffef token fix https://stackoverflow.com/questions/17912307/u-ufeff-in-python-string\n",
    "with open(\"arthurmodded.txt\", 'r', encoding=\"utf-8-sig\") as file:\n",
    "    data = file.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split file into sentence tokens\n",
    "tokenized = sent_tokenize(data)\n",
    "\n",
    "#Replace unwanted tokens and punctuation\n",
    "for i in range(len(tokenized)):\n",
    "    tokenized[i] = tokenized[i].replace('\\n', ' ')\n",
    "    tokenized[i] = tokenized[i].replace(\"\\\"\", '')\n",
    "    tokenized[i] = tokenized[i].replace('\"\"', '')\n",
    "    tokenized[i] = tokenized[i].replace('.', '')\n",
    "    tokenized[i] = tokenized[i].replace('!', '')\n",
    "    tokenized[i] = tokenized[i].replace('?', '')\n",
    "\n",
    "#Filter out lines that contain unwanted words or formats, like illustrations\n",
    "filtered = []\n",
    "for sentence in tokenized:\n",
    "    if(\"Illustration\" not in sentence):\n",
    "        filtered.append(sentence)\n",
    "\n",
    "#Stop using sentences if they exist beyond detected end of book boundary from Gutenberg\n",
    "filtered2 = []\n",
    "for sentence in filtered:\n",
    "    if(sentence[0:3] != \"***\"):\n",
    "        filtered2.append(sentence)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "#Final processing. Lowercase all sentences and append label for sentence. Label of 1 represents a fantasy example.\n",
    "final = []\n",
    "labels = []\n",
    "for sentence in filtered2:\n",
    "    final.append(sentence.lower())\n",
    "    labels.append(1)\n",
    "#print(final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "frame_data = zip(final, labels)\n",
    "\n",
    "df = pd.DataFrame(frame_data)\n",
    "df_new = df.rename(columns={0: 'Sentences', 1: 'Label'})\n",
    "df_new.to_csv('arthur.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wizard of Oz Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing newlines referenced from: https://stackoverflow.com/questions/8369219/how-to-read-a-text-file-into-a-string-variable-and-strip-newlines\n",
    "#\\uffef token fix https://stackoverflow.com/questions/17912307/u-ufeff-in-python-string\n",
    "with open(\"wonderfulwizardofozmodded.txt\", 'r', encoding=\"utf-8-sig\") as file:\n",
    "    data = file.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split file into sentence tokens\n",
    "tokenized = sent_tokenize(data)\n",
    "\n",
    "#Replace unwanted tokens and punctuation\n",
    "for i in range(len(tokenized)):\n",
    "    tokenized[i] = tokenized[i].replace('\\n', ' ')\n",
    "    tokenized[i] = tokenized[i].replace(\"\\\"\", '')\n",
    "    tokenized[i] = tokenized[i].replace('\"\"', '')\n",
    "    tokenized[i] = tokenized[i].replace('.', '')\n",
    "    tokenized[i] = tokenized[i].replace('!', '')\n",
    "    tokenized[i] = tokenized[i].replace('?', '')\n",
    "\n",
    "filtered = []\n",
    "for sentence in tokenized:\n",
    "    if(\"Illustration\" not in sentence):\n",
    "        filtered.append(sentence)\n",
    "        \n",
    "\n",
    "#Stop using sentences if they exist beyond detected end of book boundary from Gutenberg\n",
    "filtered2 = []\n",
    "for sentence in filtered:\n",
    "    if(sentence[0:3] != \"***\"):\n",
    "        filtered2.append(sentence)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "#Final processing\n",
    "final = []\n",
    "labels = []\n",
    "for sentence in filtered2:\n",
    "    final.append(sentence.lower())\n",
    "    labels.append(1)\n",
    "#print(final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "frame_data = zip(final, labels)\n",
    "\n",
    "df = pd.DataFrame(frame_data)\n",
    "df_new = df.rename(columns={0: 'Sentences', 1: 'Label'})\n",
    "df_new.to_csv('dorothy.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Irish Fairy Tales Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing newlines referenced from: https://stackoverflow.com/questions/8369219/how-to-read-a-text-file-into-a-string-variable-and-strip-newlines\n",
    "#\\uffef token fix https://stackoverflow.com/questions/17912307/u-ufeff-in-python-string\n",
    "with open(\"irishfairy.txt\", 'r', encoding=\"utf-8-sig\") as file:\n",
    "    data = file.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split file into sentence tokens\n",
    "tokenized = sent_tokenize(data)\n",
    "\n",
    "#Replace unwanted tokens and punctuation\n",
    "for i in range(len(tokenized)):\n",
    "    tokenized[i] = tokenized[i].replace('\\n', ' ')\n",
    "    tokenized[i] = tokenized[i].replace(\"\\\"\", '')\n",
    "    tokenized[i] = tokenized[i].replace('\"\"', '')\n",
    "    tokenized[i] = tokenized[i].replace('“', '')\n",
    "    tokenized[i] = tokenized[i].replace('”', '')\n",
    "    tokenized[i] = tokenized[i].replace(\"’\", \"'\")\n",
    "    tokenized[i] = tokenized[i].replace('.', '')\n",
    "    tokenized[i] = tokenized[i].replace('!', '')\n",
    "    tokenized[i] = tokenized[i].replace('?', '')\n",
    "\n",
    "#Filter out lines that contain unwanted words or formats, like illustrations\n",
    "filtered = []\n",
    "for sentence in tokenized:\n",
    "    if '\"' in sentence:\n",
    "        print(sentence)\n",
    "    if(\"Illustration\" not in sentence and \"CHAPTER\" not in sentence):\n",
    "        filtered.append(sentence)\n",
    "\n",
    "\n",
    "#Stop using sentences if they exist beyond detected end of book boundary from Gutenberg\n",
    "filtered2 = []\n",
    "for sentence in filtered:\n",
    "    if(sentence[0:3] != \"***\"):\n",
    "        filtered2.append(sentence)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "#Final processing\n",
    "final = []\n",
    "labels = []\n",
    "for sentence in filtered2:\n",
    "    final.append(sentence.lower())\n",
    "    labels.append(1)\n",
    "#print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "frame_data = zip(final, labels)\n",
    "\n",
    "df = pd.DataFrame(frame_data)\n",
    "df_new = df.rename(columns={0: 'Sentences', 1: 'Label'})\n",
    "df_new.to_csv('irishfairy.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Book of Wonder Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing newlines referenced from: https://stackoverflow.com/questions/8369219/how-to-read-a-text-file-into-a-string-variable-and-strip-newlines\n",
    "#\\uffef token fix https://stackoverflow.com/questions/17912307/u-ufeff-in-python-string\n",
    "with open(\"bookofwonder.txt\", 'r', encoding=\"utf-8-sig\") as file:\n",
    "    data = file.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split file into sentence tokens\n",
    "tokenized = sent_tokenize(data)\n",
    "\n",
    "#Replace unwanted tokens and punctuation\n",
    "for i in range(len(tokenized)):\n",
    "    tokenized[i] = tokenized[i].replace('\\n', ' ')\n",
    "    tokenized[i] = tokenized[i].replace(\"\\\"\", '')\n",
    "    tokenized[i] = tokenized[i].replace('\"\"', '')\n",
    "    tokenized[i] = tokenized[i].replace('“', '')\n",
    "    tokenized[i] = tokenized[i].replace('”', '')\n",
    "    tokenized[i] = tokenized[i].replace(\"’\", \"'\")\n",
    "    tokenized[i] = tokenized[i].replace('.', '')\n",
    "    tokenized[i] = tokenized[i].replace('!', '')\n",
    "    tokenized[i] = tokenized[i].replace('?', '')\n",
    "\n",
    "#Filter out lines that contain unwanted words or formats, like illustrations\n",
    "filtered = []\n",
    "for sentence in tokenized:\n",
    "    if(\"Illustration\" not in sentence and \"CHAPTER\" not in sentence and not sentence.isupper()):\n",
    "        filtered.append(sentence)\n",
    "\n",
    "#Stop using sentences if they exist beyond detected end of book boundary from Gutenberg\n",
    "filtered2 = []\n",
    "for sentence in filtered:\n",
    "    if(sentence[0:3] != \"***\"):\n",
    "        filtered2.append(sentence)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "#Final processing\n",
    "final = []\n",
    "labels = []\n",
    "for sentence in filtered2:\n",
    "    final.append(sentence.lower())\n",
    "    labels.append(1)\n",
    "#print(final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "frame_data = zip(final, labels)\n",
    "\n",
    "df = pd.DataFrame(frame_data)\n",
    "df_new = df.rename(columns={0: 'Sentences', 1: 'Label'})\n",
    "df_new.to_csv('bookofwonder.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**World of Ice and Fire Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing newlines referenced from: https://stackoverflow.com/questions/8369219/how-to-read-a-text-file-into-a-string-variable-and-strip-newlines\n",
    "#\\uffef token fix https://stackoverflow.com/questions/17912307/u-ufeff-in-python-string\n",
    "with open(\"iceandfire.txt\", 'r', encoding=\"utf-8-sig\") as file:\n",
    "    data = file.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split file into sentence tokens\n",
    "tokenized = sent_tokenize(data)\n",
    "\n",
    "#Replace unwanted tokens and punctuation\n",
    "for i in range(len(tokenized)):\n",
    "    tokenized[i] = tokenized[i].replace('\\n', ' ')\n",
    "    tokenized[i] = tokenized[i].replace(\"\\\"\", '')\n",
    "    tokenized[i] = tokenized[i].replace('\"\"', '')\n",
    "    tokenized[i] = tokenized[i].replace('“', '')\n",
    "    tokenized[i] = tokenized[i].replace('”', '')\n",
    "    tokenized[i] = tokenized[i].replace(\"’\", \"'\")\n",
    "    tokenized[i] = tokenized[i].replace('.', '')\n",
    "    tokenized[i] = tokenized[i].replace('!', '')\n",
    "    tokenized[i] = tokenized[i].replace('?', '')\n",
    "    \n",
    "#Filter out lines that contain unwanted words or formats, like illustrations\n",
    "filtered = []\n",
    "for sentence in tokenized:\n",
    "    if(\"illustration\" not in sentence and \"CHAPTER\" not in sentence and len(sentence) > 0):\n",
    "        filtered.append(sentence)\n",
    "\n",
    "#Stop using sentences if they exist beyond detected end of book boundary from Gutenberg\n",
    "filtered2 = []\n",
    "for sentence in filtered:\n",
    "    if(sentence[0:3] != \"***\"):\n",
    "        filtered2.append(sentence)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "#Final processing\n",
    "final = []\n",
    "labels = []\n",
    "for sentence in filtered2:\n",
    "    final.append(sentence.lower())\n",
    "    labels.append(1)\n",
    "#print(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "frame_data = zip(final, labels)\n",
    "\n",
    "df = pd.DataFrame(frame_data)\n",
    "df_new = df.rename(columns={0: 'Sentences', 1: 'Label'})\n",
    "df_new.to_csv('iceandfire.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
